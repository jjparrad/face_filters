{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VnV7jk7N-TPu",
    "outputId": "04cc5a04-f89f-44dd-a8c3-341408466916"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vXKZt9Ui8Il-",
    "outputId": "f0d41f40-cee2-4d69-9b70-8236b2b2f601"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"jessicali9530/lfw-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJedfGIo9XHg"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18yaVGeY9Epi"
   },
   "outputs": [],
   "source": [
    "# Load face detection classifier\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgC_gniq9aWW"
   },
   "outputs": [],
   "source": [
    "# # This takes like 8 minutes\n",
    "\n",
    "latent_dim = 128\n",
    "image_size = 64\n",
    "images = []\n",
    "\n",
    "# Preprocess images\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(root, file)\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            # Handle case where image could not be loaded\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not load image {img_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Convert to grey for face detection\n",
    "            grey_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Call classifier on image to detect faces of any size\n",
    "            face = face_classifier.detectMultiScale(\n",
    "                grey_img, scaleFactor=1.1 , minNeighbors=5 , minSize=(40,40)\n",
    "            )\n",
    "\n",
    "            # Process and append images only if faces are detected\n",
    "            if len(face) > 0:\n",
    "                # Iterate through all detected faces (or pick the first one)\n",
    "                for (x, y, w, h) in face:\n",
    "                    x_max = x+w\n",
    "                    y_max = y+h\n",
    "                    # Ensure crop coordinates are within image bounds\n",
    "                    x = max(0, x)\n",
    "                    y = max(0, y)\n",
    "                    x_max = min(img.shape[1], x_max)\n",
    "                    y_max = min(img.shape[0], y_max)\n",
    "\n",
    "                    if w > 0 and h > 0: # Ensure valid crop dimensions\n",
    "                        im_cropped = img[y:y_max , x:x_max]\n",
    "\n",
    "                        # Ensure im_cropped is not empty after cropping\n",
    "                        if im_cropped.size == 0:\n",
    "                            print(f\"Warning: Empty crop for {img_path}. Skipping.\")\n",
    "                            continue\n",
    "\n",
    "                        im_cropped = cv2.cvtColor(im_cropped, cv2.COLOR_BGR2RGB)\n",
    "                        im_cropped = cv2.resize(im_cropped, (image_size, image_size))\n",
    "                        im_cropped = im_cropped.astype(\"float32\") / 255.0\n",
    "                        images.append(im_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pa7933SP920e",
    "outputId": "0eb8d521-f816-4d59-836c-07c11e15fb91"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "images = np.array(images)\n",
    "x_train, x_test = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training shape:\", x_train.shape)\n",
    "print(\"Test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzqEyxwP-Our"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Create encoder\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Input((64, 64, 3)),\n",
    "    keras.layers.Conv2D(32, (3,3), activation='relu', padding='same', strides=2),\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu', padding='same', strides=2),\n",
    "    keras.layers.Conv2D(128, (3,3), activation='relu', padding='same', strides=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'), # I think that if we change the image size we have to change this value\n",
    "    keras.layers.Dense(latent_dim * 2)\n",
    "], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "# Create decoder\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.Input((latent_dim,)),\n",
    "    keras.layers.Dense(8 * 8 * 128, activation='relu'),\n",
    "    keras.layers.Reshape((8, 8, 128)),\n",
    "    keras.layers.Conv2DTranspose(128, (3,3), activation='relu', padding='same', strides=2),\n",
    "    keras.layers.Conv2DTranspose(64, (3,3), activation='relu', padding='same', strides=2),\n",
    "    keras.layers.Conv2DTranspose(32, (3,3), activation='relu', padding='same', strides=2),\n",
    "    keras.layers.Conv2D(3, (3,3), activation='sigmoid', padding='same')\n",
    "], name=\"decoder\")\n",
    "decoder.summary()  \n",
    "\n",
    "\n",
    "# VAE creation\n",
    "def sample_latent(z):\n",
    "    z_mean, z_log_var = tf.split(z, num_or_size_splits=2, axis=1)\n",
    "    epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "    z = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    return z, z_mean, z_log_var\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass para predicciÃ³n / validation\n",
    "        z_all = self.encoder(inputs)\n",
    "        z, _, _ = sample_latent(z_all)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction\n",
    "\n",
    "\n",
    "# UNUSED !!!!! :(\n",
    "# Kept for evil spirits\n",
    "def vae_loss(y_true, y_pred):\n",
    "    # ReconstrucciÃ³n ðŸ‘·\n",
    "    reconstruction_loss = tf.reduce_mean(tf.keras.losses.mse(y_true, y_pred))\n",
    "\n",
    "    # KL divergence\n",
    "    z_all = vae.encoder(y_true)\n",
    "    z_mean, z_log_var = tf.split(z_all, num_or_size_splits=2, axis=1)\n",
    "    kl_loss = -0.5 * tf.reduce_mean(\n",
    "        tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "    )\n",
    "\n",
    "    return reconstruction_loss + kl_loss\n",
    "\n",
    "# Damned souls (red)\n",
    "# CLOSER THAN THE OTHER ONE \n",
    "# https://www.tensorflow.org/tutorials/generative/cvae\n",
    "def compute_loss(y_true, y_pred):\n",
    "    z_all = vae.encoder(y_true)\n",
    "    z,z_mean,z_log_var = sample_latent(z_all)\n",
    "    # z_mean, z_log_var = tf.split(z, num_or_size_splits=2, axis=1)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred, labels=y_true)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, z_mean, z_log_var)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "        axis=raxis)\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam(), loss=compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7QsB2f1K-mzR",
    "outputId": "d9e20fa8-9e16-43d4-af3d-e6144a5bb6f6"
   },
   "outputs": [],
   "source": [
    "# Train the vae\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "vae.fit(\n",
    "    x_train, x_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_test, x_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "T1wU0TLN-pdt",
    "outputId": "9c263b74-3e33-4124-e85f-ed40168ac90c"
   },
   "outputs": [],
   "source": [
    "# Compare original and reconstructed images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n = 10\n",
    "z_all = vae.encoder(x_test[:n])\n",
    "z, _, _ = sample_latent(z_all)\n",
    "decoded_imgs = vae.decoder(z)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Reconstructed\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
