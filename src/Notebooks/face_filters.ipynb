{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VnV7jk7N-TPu",
    "outputId": "04cc5a04-f89f-44dd-a8c3-341408466916"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vXKZt9Ui8Il-",
    "outputId": "f0d41f40-cee2-4d69-9b70-8236b2b2f601"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "# path = kagglehub.dataset_download(\"jessicali9530/lfw-dataset\")\n",
    "path = kagglehub.dataset_download(\"jangedoo/utkface-new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJedfGIo9XHg"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18yaVGeY9Epi"
   },
   "outputs": [],
   "source": [
    "# Load face detection classifier\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "image_size = 128\n",
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgC_gniq9aWW"
   },
   "outputs": [],
   "source": [
    "# # This takes like 8 minutes\n",
    "\n",
    "\n",
    "i = 0\n",
    "# Preprocess images\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if i >= 13000:\n",
    "            break\n",
    "        \n",
    "        if file.endswith(\".jpg\"):\n",
    "            i+=1\n",
    "            img_path = os.path.join(root, file)\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            # Handle case where image could not be loaded\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not load image {img_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Convert to grey for face detection\n",
    "            grey_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Call classifier on image to detect faces of any size\n",
    "            face = face_classifier.detectMultiScale(\n",
    "                grey_img, scaleFactor=1.1 , minNeighbors=5 , minSize=(40,40)\n",
    "            )\n",
    "\n",
    "            # Process and append images only if faces are detected\n",
    "            if len(face) > 0:\n",
    "                # Iterate through all detected faces (or pick the first one)\n",
    "                for (x, y, w, h) in face:\n",
    "                    x_max = x+w\n",
    "                    y_max = y+h\n",
    "                    # Ensure crop coordinates are within image bounds\n",
    "                    x = max(0, x)\n",
    "                    y = max(0, y)\n",
    "                    x_max = min(img.shape[1], x_max)\n",
    "                    y_max = min(img.shape[0], y_max)\n",
    "\n",
    "                    if w > 0 and h > 0: # Ensure valid crop dimensions\n",
    "                        im_cropped = img[y:y_max , x:x_max]\n",
    "\n",
    "                        # Ensure im_cropped is not empty after cropping\n",
    "                        if im_cropped.size == 0:\n",
    "                            print(f\"Warning: Empty crop for {img_path}. Skipping.\")\n",
    "                            continue\n",
    "\n",
    "                        im_cropped = cv2.cvtColor(im_cropped, cv2.COLOR_BGR2RGB)\n",
    "                        im_cropped = cv2.resize(im_cropped, (image_size, image_size))\n",
    "                        im_cropped = im_cropped.astype(\"float32\") / 255.0\n",
    "                        images.append(im_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pa7933SP920e",
    "outputId": "0eb8d521-f816-4d59-836c-07c11e15fb91"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "images = np.array(images)\n",
    "x_train, x_test = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training shape:\", x_train.shape)\n",
    "print(\"Test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(128, 128, 3))\n",
    "\n",
    "x = layers.Conv2D(16, 3, strides=2, padding=\"same\", activation=\"relu\")(encoder_inputs)\n",
    "x = layers.Conv2D(32, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv2D(64, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv2D(128, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "\n",
    "@register_keras_serializable()\n",
    "def sampling(args):\n",
    "    mean, log_var = args\n",
    "    epsilon = tf.random.normal(shape=tf.shape(mean))\n",
    "    return mean + tf.exp(0.5 * log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(latent_dim,))\n",
    "x = layers.Dense(8 * 8 * 128, activation=\"relu\")(decoder_inputs)\n",
    "x = layers.Reshape((8, 8, 128))(x)\n",
    "\n",
    "# UpSampling + Conv blocks (reduce checkerboard / mejora detalles)\n",
    "x = layers.UpSampling2D()(x)\n",
    "x = layers.Conv2D(128, 3, padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = layers.UpSampling2D()(x)\n",
    "x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = layers.UpSampling2D()(x)\n",
    "x = layers.Conv2D(32, 3, padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = layers.UpSampling2D()(x)\n",
    "x = layers.Conv2D(16, 3, padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "# Salida final en [0,1] usando sigmoid (mÃ¡s directo para MAE/MSE)\n",
    "decoder_outputs = layers.Conv2D(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = Model(decoder_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzqEyxwP-Our"
   },
   "outputs": [],
   "source": [
    "beta = 0.001\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(x, training=True)\n",
    "            reconstruction = self.decoder(z, training=True)  # ya estÃ¡ en [0,1] por sigmoid\n",
    "\n",
    "            # Reconstruction losses: MAE + SSIM\n",
    "            mae = tf.reduce_mean(tf.abs(x - reconstruction), axis=[1,2,3])  # per-sample\n",
    "            ssim = tf.clip_by_value(tf.image.ssim(x, reconstruction, max_val=1.0), 0.0, 1.0)\n",
    "            ssim_loss = 1.0 - ssim  # per-sample\n",
    "            recon_loss = mae + 0.5 * ssim_loss\n",
    "            recon_loss = tf.reduce_mean(recon_loss)  # scalar\n",
    "\n",
    "            # KL divergence\n",
    "            kl_loss = -0.5 * tf.reduce_sum(\n",
    "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1\n",
    "            )\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "            total_loss = recon_loss + beta * kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        return {\"loss\": total_loss,\n",
    "                \"reconstruction_loss\": recon_loss,\n",
    "                \"kl_loss\": kl_loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x = data\n",
    "        z_mean, z_log_var, z = self.encoder(x, training=False)\n",
    "        reconstruction = self.decoder(z, training=False)\n",
    "\n",
    "        # Compute losses again, same as train_step (without gradients)\n",
    "        mae = tf.reduce_mean(tf.abs(x - reconstruction), axis=[1, 2, 3])\n",
    "        ssim = tf.clip_by_value(tf.image.ssim(x, reconstruction, max_val=1.0), 0.0, 1.0)\n",
    "        ssim_loss = 1.0 - ssim\n",
    "        recon_loss = tf.reduce_mean(mae + 0.5 * ssim_loss)\n",
    "\n",
    "        kl_loss = -0.5 * tf.reduce_sum(\n",
    "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1\n",
    "        )\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "        total_loss = recon_loss + beta * kl_loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": recon_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "\n",
    "    def call(self, x):\n",
    "        _, _, z = self.encoder(x)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam(), loss=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveEveryN(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            vae.build((None,) + x_train.shape[1:])\n",
    "            print(f\"\\nðŸ”½ Saving VAE on epoch {epoch+1}...\")\n",
    "            vae.encoder.save(f\"../Models/Temp/vae_encoder_epoch_{epoch+1}.keras\")\n",
    "            vae.decoder.save(f\"../Models/Temp/vae_decoder_epoch_{epoch+1}.keras\")\n",
    "            vae.save_weights(f'../Models/Temp/vae_weights_epoch_{epoch+1}.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7QsB2f1K-mzR",
    "outputId": "d9e20fa8-9e16-43d4-af3d-e6144a5bb6f6"
   },
   "outputs": [],
   "source": [
    "# Train the vae\n",
    "epochs = 300\n",
    "batch_size = 128\n",
    "\n",
    "vae.fit(\n",
    "    x_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_test, None),\n",
    "    callbacks=[SaveEveryN()]\n",
    ")\n",
    "vae.build((None,) + x_train.shape[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.encoder.save( \"../Models/vae_encoder_300epochs.keras\")\n",
    "vae.decoder.save( \"../Models/vae_decoder_300epochs.keras\")\n",
    "\n",
    "vae.save_weights('../Models/vae_weights_300epochs.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = keras.models.load_model(\"../Models/vae_encoder_300epochs.keras\", compile=False, safe_mode=False)\n",
    "decoder = keras.models.load_model(\"../Models/vae_decoder_300epochs.keras\", compile=False, safe_mode=False)\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "vae.build((None,) + x_train.shape[1:])\n",
    "vae.load_weights(\"../Models/vae_weights_300epochs.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "T1wU0TLN-pdt",
    "outputId": "9c263b74-3e33-4124-e85f-ed40168ac90c"
   },
   "outputs": [],
   "source": [
    "# Compare original and reconstructed images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10\n",
    "\n",
    "# El encoder devuelve: z_mean, z_log_var, z\n",
    "z_mean, z_log_var, z = vae.encoder(x_test[:n])\n",
    "decoded_imgs = vae.decoder(z)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Reconstructed\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = vae.history.history['loss']\n",
    "epoch_range = range(epochs)\n",
    "\n",
    "plt.plot(epoch_range, loss, label='Training Loss')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
